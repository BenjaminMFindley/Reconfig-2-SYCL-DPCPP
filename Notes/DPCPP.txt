DPC++ is oneAPI's implementation of SYCL.
DPC++ is described as a "SYCL-aware C++ compiler"
"SYCL is a single source where host code and heterogeneous accelerator kernels can be mixed in same source files."
"Every DPC++ (or SYCL) program is also a C++ program. Neither SYCL
nor DPC++ relies on any language changes to C++. Both can be fully
implemented with templates and lambda functions."


Kernel code = code to be run on a device
"The kernel class encapsulates methods and data for executing code on the device when a command group is instantiated. Kernel object is not explicitly constructed by the user and is constructed when a kernel dispatch function, such as parallel_for, is called"

kernel code usually takes the form of lambda functions. Such lambda functions must capture variables by value [=]. SYCL kernel functions always have a return type of void.

More specialized kernel code can be written for specific types of accelerators or specific models. In addition to a main, generic kernel.

QUEUES

"A queue is an abstraction to which actions are submitted for execution on a single device"
"A queue is bound to a single device, and that binding occurs on
construction of the queue. It is important to understand that work
submitted to a queue is executed on the single device to which that queue
is bound. Queues cannot be mapped to collections of devices because that
would create ambiguity on which device should perform work. Similarly,
a queue cannot spread the work submitted to it across multiple devices.
Instead, there is an unambiguous mapping between a queue and the
device on which work submitted to that queue will execute"
"Selection of the device when
constructing a queue is achieved through a device selector abstraction and
associated device_selector class."

We'll start by creating a queue. Queues handle the direction of work to be done on accelerators. Tasks are submitted to the queue and are then offloaded to a device. The host then continues execution of the program while the device performs the task asynchronously.
```queue q;```
Devices such as CPU, GPU, and FPGA can be specified through the selector.
```
  gpu_selector selector;
  //cpu_selector selector;
  //default_selector selector;
  //host_selector selector;
  //accelerator_selector selector;
  //fpga_selector ??? CL/sycl/intel/fpga_extensions.hpp, not available in SYCL natively???
  
  queue q(selector);
```
or ```queue q(gpu_selector{})```
The default "q" option for queue uses the default selector, which selects the most capable device available at runtime.

The device class has a function **get_info** which gives information about the device.

```std::cout << "Device Name: " << q.get_device().get_info<info::device::name>() << "\n";```
```std::cout << "Device Vendor: " << q.get_device().get_info<info::device::vendor>() << "\n";```

"By creating Buffers and Accessors, DPC++ ensures that the data is available to host and device without any effort on your part. DPC++ also allows you explicit control over data movement to achieve best peformance."

CUSTOM DEVICE SELECTION:
The default device selectors such as gpu_selector will select one of the available devices in the class. This is useful to get development moving quickly, but in most applications, you will want to select specific devices for specific tasks.
All device selectors are derived from the device_selector base class:
```
virtual int operator()(const device &dev) const {
; /* Device selection logic */
}
```

operator() is run on each available device and returns an integer score to determine which device should be selected. The device that returns the highest value is selected. Devices which return negative values will never be chosen.

The user is free to define whatever logic they want to determine this integer score, allowing for an arbitrarily complex selection process.

One simple way to select a particular device is to search for particular strings within the device name or vendor information. For example:

```
class my_selector : public device_selector {
  public:
    int operator()(const device &dev) const override {
      if (
        dev.get_info<info::device::name>().find("Arria")
          != std::string::npos &&
        dev.get_info<info::device::vendor>().find("Intel")
          != std::string::npos) {
      return 1;
    }
      return -1;
    }
};
```

Such a simple selection method will be sufficient for our purposes, but selection logic can be arbitrarily complex.


DEVICE WORK
See task graph: pp 48 (work dependencies)

All mechanisms used to submit work to a device are members of the **handler** class or the **queue** class (??).
Work is submitted to devices in the form of command groups. Command groups include:
1. exactly one action (and no more), which is either device code submitted for execution or manual memory operations such as **copy**.
2. Host code that defines dependences which restrict when asynchronous execution of the submitted work can begin. For example: creation of accessors or buffers

 See pp 54 for table of handler member functions


Example of submitting work to a queue, Q:
```
Q.submit([&](handler& h) {			// function called on host
  accessor acc{B, h};				// host code defining accessor, setting up dependencies

  h.parallel_for(size , [=](auto& idx) {
    acc[idx] = idx;				// Device code to be run when runtime dependencies are met
  });
});
```


DATA MANAGEMENT:
Running heterogeneous computing systems efficiently requires careful handling of data. It is essential for data to be available for accelerator execution as promptly as possible, as any time the device sits idle is a great deal of potential performance wasted.

There are two methods for managing data in DPC++: Unified Shared Memory (USM) and buffers.

Device code requires data as an input and will return its own data as an output. Devices also often have their own memory which is distinct from the host's, and cannot be directly accessed by the host. An important question is how to safely and efficiently handle the storage and movement of data between the host and discrete devices. Synchronization and coherency are potential problems

Accesses to directly-attached memory are __local__ accesses. Accesses to another device's memory are __remote__ accesses. Local accesses are much faster than remote accesses, so it is typically desirable for a device to utilize its local memory for assigned computation. This may require manual movement of data between different memory pools to insure it is within the device's local scope.

Memory management can either be explicit or implicit. Data may be explicitly copied within the program itself, or implicitly by the runtime.

The compiler can handle data management on its own, but for best performance, it is often required for the programmer to define the movement and storage of data manually. For example, one effective strategy is to transfer data while the device (? not CPU?) is busy with computation.
However, explicit data management can also be very time consuming and error prone. You will likely only want to explicitly manage memory accesses that are of the most importance to performance, if at all.

Implicit memory management will assure that all data required by the kernel is transferred to the appropriate location before execution but does not allow control of when this transfer occurs, leading to potentional loss in performance.

USM, BUFFERS, IMAGES:
USM is a pointed-based memory management system, similar to C/C++.
Buffers are an abstraction of memory implemented as 1-3D arrays and are accessible by both host and device. Access of a buffer must be done through **accessor** objects.
Images are a special implementation of buffers optimized for use in image processing. They support special image formats as well as reading of images using sampelr objects (whatever that means).
The interface for buffers and images are mostly the same, so only buffers will be covered.

USM:
USM is pointer-based and works similar to **malloc** or **new** in C/C++. USM defines a unified virtual address space that is shared between host and devices. A pointer is valid both on the host and any devices, so no translation between them is necessary.
USM defines three types of allocations: device, host, and shared. All allocations are performed by the host. (See Figure 3-3, pg 67 in book for table of characteristics)
Device allocations are located on device-attached memory and are only accessible via device. Data must be copied explicitly in order to move between host and device memory.
Both host and shared allocations are accessible by either host or device. Host allocations are located in host memory. Data accessed on host memory by devices do not transfer into local device memory. The data is instead sent via bus, such as PCI-E (or AXI?).
Shared memory is like host memory in that it can be accessed by both host and device, however, shared memory can migrate between host and device-local memory. This allows for much faster execution by the device after the migration has completed. This migration occurs automatically via the runtime and lower-level drivers.

Device allocations are explicit and are accomplished via the memcpy() function, which is part of the queue and handler classes.
Any instances of malloc_device must be explicitly copied to the device inside kernel code using the memcpy() function, whereas malloc_host and malloc_shared are implicit and migration is handled by the runtime, with no explicit migration required by the programmer. (See Figures 3-4 and 3-5, pp 69-70)

BUFFERS:
Buffers are a data abstraction of a certain C++ type. Buffer elements can be scalar data types (int, float, double), vectors, or another user-defined class/structure. "Data structures in buffers must be C++ trivially copyable, which means that an object can be safely copied byte by byte where copy constructors do not need to be invoked."
As buffers represent data objects rather than specific memory addresses, they cannot be accessed like regular C++ arrays "". A single buffer object may be distributed between multiple discrete memory locations on different devices. For this reason, accessor objects are required to read from and write to buffers.

An empty buffer may be constructed by specifyng a range for the size of the buffer. Data must later be initialized inside the buffer before it can be read from.
Alternatively, existing host data may be used to initialize a new buffer by invoking one of the constructors that takes a pointer to an existing host allocation (or "a set of InputIterators, or a container that has certain properties").
Buffers may also be created from an existing cl_mem object if using SYCL OpenCL compatibility features.

ACCESSORS:
Accessors are the only way in which the host or device may read from or write to buffers. Accessors can be instantiated with either read, write, or read_write access modes. read_write is the default.
Using appropriate access modes when creating accessors is important as it provides implicit information used to help the runtime manage memory. For example, creating an accessor with the read mode tells the runtime that it does not need to copy memory back to the host, as the device has not changed it.
Appropriate use of access modes will help the runtime optimize kernel scheduling and data migration.

!!!Maybe add some slides about task graphs/data ordering

Queues may be instantiated with the "in-order" property, which will result in the queue executing its tasks in the order in which they are submitted.
```queue Q{property::queue::in_order()};```
This is simple and intuitive, and makes it easy to organize tasks by dependency, but not allowing the runtime the flexibility to run tasks in whichever order is more efficient can result in reduced performance.
By default, queues are considered "out-of-order", meaning they do not necesszrily run tasks in order of submission. This means that we must specify any dependencies between tasks ourselves to ensure correct functionality. This is done using "command groups".

COMMAND GROUPS:
"A command group is an object that specifies a task and its dependences. Command groups are typically written as C++ lambdas passed as an argument to the submit() method of a queue object. This lambdaâ€™s only parameter is a reference to a handler object. The handler object is used inside the command group to specify actions, create accessors, and specify dependences."

Dependencies can either be explicit or implicit. Explicit dependencies refer to dependencies between computations. This form of dependency is most relevant to code utilizing USM for data management and are specified using events.
Implicit dependencies refer to depdendencies between data accesses. This form of dependency is used in code that utilizes buffers for data management, as depdendencies are specified through accessors.

EVENTS (COMPUTATIONAL DEPENDENCIES):
The queue's submit() function returns an event object. This event object can be captured and referenced for other purposes such as wait() or depends_on().
depends_on() is a handler function and can be used to specify events on which the current operation depends. depends_on() takes an event or vector of events and will wait until all specified events have finished before continuing execution on the device.
```
// Task A
auto eventA = Q.submit([&](handler &h) {
h.parallel_for(N, [=](id<1> i) { /*...*/ });
});
// Task B
auto eventB = Q.submit([&](handler &h) {
h.depends_on(eventA);
h.parallel_for(N, [=](id<1> i) { /*...*/ }); // Will not execute until eventA has finished
});
```

DATA DEPENDENCIES: Data dependencies exist in one of three relationships.
Read-after-write: When task B must wait for task A to write data before it can read.
Write-after-read: When task B must wait for task A to read data before it can write.
Write-after-write: When task B must wait for task A to write data before it can write.

These relationships are specified implicitly by accessor access modes and task ordering. If two different tasks have accessors for the same data, whichever task is submitted first will be executed first, and an implicit dependency is created based on the types of access.
(See Figure 3-13, pg 81 for code example. Also 3-15, pg 84)

NOW ON: Chapter 4: Expressing Parallelism, pg 91